\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{listingsutf8}
\usepackage[a4paper, top = 1in, bottom = 1in, left=1.5in,right=1.5in]{geometry}
\usepackage[backend=biber, citestyle=ieee]{biblatex}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{enumitem}

\newtheorem{definition}{Definition}[section]


\bibliography{ai.bib}

\title{Notes for AI}
\author{Peter Heilbo Ratgen}
\date{\today}

\hfuzz=80pt


\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Week 5 February - Introduction}%
\subsection{Basics}
The course is an introduction to the basics of Artificial Intelligence. We will
get an overview of the base of the artificial intelligence methods. We will use
python as a programming language. Labs and support will be done in python.
Prerequisite to the exam is to complete the homework of the lectures.
\cite{book:artificial_intelligence_modern_approach}

You should help each other, but coding should be done individually.
\begin{itemize}
  \item Thinking humanly
  \item Acting humanly
  \subitem The Turing test is used to test this. The longer a human can be
  fooled into thing that the human is talking to a human, and not a bot.  This
  could be chatbots acting humanly. You have to test for:
  \begin{itemize}
    \item Natural language processing
    \item Knowledge representation
    \item Automaed reasoning\
    \item Machine learning
  \end{itemize}
    Success depends on deception. Chatbot can use cheap tricks. Mitzuku has
    recently won for the 5th time.
    Computers have a had time with multiple choice questions. Eg the "The large
    ball crashed right through the table because it was made of styrofoam". If
    you replace "styrofoam" with "steel", then the answer is totally different.
    
    \textbf{A better test?} A better Turing test, would be one that can be
    administed and graded by a machine, and more objectivity in the it would not
    depend of subjectity of humans.

  \item Thinking rationally
    \subitem
    It is about the idealized or "right" way of thinking. It is hard to describe
    the world using logical notation. The procedure of applying these local
    statements and deducing them. 
    We also have a though time dealing with uncertainty, representing the gray
    areas.
  \item Acting rationally
    \subitem
    Acting rationally is acting with the goal of achieving the goal, that has
    been set. Utility is about the goal that has been set, whether it is about,
    shortest route, least time or fewest changes in a public transit system.
\end{itemize}
\subsection{Successes of AI}
\begin{itemize}
  \item IBM Watson is an AI created by IBM. IBM is one of the companies that has been
investing in AI for the longest times. 
  \item Self driving cars is one of the successes of AI. This is an example of a
  rational acting. 
  \item Natural language processing is also a great improvement, with speech
technologies and machine translation.
  \item Vision, OCR, handwriting recognition and face detection and recognition.
  \item Mathematics, program solved unsolved conjecture. Also wolfram alpha.
  \item Games
    \subitem Chess(champion beaten in 1997), checkers(solved in 2007), Go
    (beaten for the first time by a Google AI), Google AI beat top StarCraft
    players.
  \item Logistics, scheduling, planning.
    \subitem A lot of the advancement are done by the military. In the 1991 Gulf
    War an AI planned and scheduled for 50,000 vehicles and such.
  \item Robotics
    \subitem Mars rovers, self driving cars, drones, robot soccer, personal
    robotics.
\end{itemize}



Exercises will start at 10:20.
\paragraph{Basics}
Python code is fairly simple and readable. Beginning and ending of blocks is
done purely by indentation. We will use the Python Console for trying out
examples. Variables can change types throughout the program. A variable can
start as a string end as float. We can use \texttt{+} for string concatenation.
We can use triples quotes for strings containing both \texttt{'} and \texttt{"}.

Variables in Python do not have intrinsic types. But assignment does not create
copies, but references. References are deleted by the garbage collector, when
the reference has passed out of scope. Names cannot start with numbers.

We can have multiple assignments. And swapping vars is easy.
\begin{lstlisting}
>>> x, y = 2, 3
>>> y
3
>>> x, y = y, x
>>> y
2
>>> x
3
\end{lstlisting}

\subparagraph{Sequence types}
Sequence types are tuples, strings and lists. In a tuple we can have multiple
types of variables. Tuples are immutable, such that they cannot be changed after
it has been created. Strings are also immutable. Lists are mutable, they can
also have mixed types.
These sequence types have much syntax in common. If we have to change elements
in an immutable tuple or string a new copy has to be created. Lists can be
shrunk or expanded as you go. We assign some different variables:

\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> tu = (23, 'abc', 4.56, (2,3), 'def')
>>> tu
(23, 'abc', 4.56, (2, 3), 'def')
>>> li = ['abc', 34, 4.34, 23]
>>> st = "Hello World"
>>> st
'Hello World'
>>> st = Helllo wordl'
  File "<stdin>", line 1
    st = Helllo wordl'
                ^
SyntaxError: invalid syntax
>>> st = 'Helllo wordl'
>>> st
'Helllo wordl'
>>> st = """THis is a multiple line
... string that uses triple quotes"""
>>> st
'THis is a multiple line\nstring that uses triple quotes'
\end{lstlisting}
We can also have negative indexes, such that -1 is the last character:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> st[-1]
's'
>>> st[-2]
'e'
\end{lstlisting}
If want to get the 3 middle elements of the tuple we defined:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> tu[1:4]
('abc', 4.56, (2, 3))
\end{lstlisting}
From this we get a \underline{copy} of the selected part of the tuple. If we do
not specify where in the tuple to start, we start from the beginning.
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> tu[:3]
(23, 'abc', 4.56)
\end{lstlisting}
We can do a copy like this:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> tu[:]
(23, 'abc', 4.56, (2, 3), 'def')
\end{lstlisting}

In this example there is a big difference between line 3 and 4.
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> l3 = ['4', '5']
>>> l4 = ['6', '7']
>>> l3 = l4
>>> l3 = l4[:]
\end{lstlisting}
In line 3 we assign \texttt{l3} we assign to the reference to \texttt{l4}. In
the 4th line we assign \texttt{l3} to a \underline{copy} of \texttt{l4}, such
that changes in \texttt{l4} will not be reflected in \texttt{l3}.

We can use the \texttt{in} operator to check if we have a substring. We can
concat tuples:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> tu[:] + tu2[:]
(23, 'abc', 4.56, (2, 3), 'def', 12, 'yeet')
>>> tu + tu2
(23, 'abc', 4.56, (2, 3), 'def', 12, 'yeet')
\end{lstlisting}

\subparagraph{Dictionaries}
Dictionaries can store a mapping between a set of keys and values. We can
create a dictionary with:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> d = {'user' : 'bozo', 'pswd' : 1234}
\end{lstlisting}
The values can be anything. We can get the value of a key like this:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> d['user']
'bozo'
\end{lstlisting}
We can delete a key:value pair like this:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\ttfamily,
language=python, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
>>> del d['pswd']
>>> d
{'user': 'bozo'}
\end{lstlisting}

\newpage

\section{Week 7 - Intelligent Agents}
An agent is anything that can be viewed as perceiving its environment through
sensors and acting upon that environment through actuators. We can say that
an agents behaviour is described by its \textbf{agent function}. This maps any sequence
to an action. This is a mathematical abstraction, the \textit{actual program} is
called the \textbf{agent program}. \cite[p. 3]{presentation:intelligent_agents}

We can have many types of agents, these can perceive in many ways: here are some
examples.
\begin{itemize}
  \item Human agent
    \subitem Eyes, ears, nose
  \item Robotic agent
    \subitem Cameras, and infrared range finders
  \item Software agent
    \subitem Keystrokes, file contents \& network packets.
\end{itemize}

\subsection{Vaccum-cleaner world}%
\label{sub:vaccum_cleaner_world}
A vecuum-cleaner has two locations to take care of: A and B. It has four
actions: left, right, such and NoOp. We can have a simple programme for cleaning
with this vaccum:

\begin{lstlisting}
if status = Dirty then return Suck
else if Location = A then return Right
else if Location = B then return Left
\end{lstlisting}

The vacuum-cleaner is a rational agent, it tries to optimize on a performance
measure. The choice of performance measure is a critical one. "Doing the right
thing" is to always act according to the performance measure. We also call the
performace measure the utility function, it is an objetive criteria for the
success of an agent's behaviour.

An example of potential performance measures for the vacuum agent.
\begin{itemize}
  \item amount of dirt cleaned up
  \item amount of time taken,
  \item amount of electricity consumed
  \item amount of noise generated
\end{itemize}
It would be desirable to have the agent take as little time as possible, but if
we only consider this, then it would stand still. But if combined, with eg the
amount of dirt cleaned up, then we could have beter operation.

\subsection{Autonomy}%
\label{sub:autonomy}
An autonomous agnet always has the ability to learn and adapt, it can always say
"no", it also needs enough built-in knowledge to survive.

\paragraph{Task Environment Specification}
Problem specification:
\begin{itemize}
  \item Performace meassure
  \item Environment
  \item Actuators
  \item Sensors
\end{itemize}
For a autonomous taxi
\begin{itemize}
  \item Performace meassure
    \subitem Safe, fastm, legal
  \item Environment
    \subitem Roads
  \item Actuators
    \subitem Steering wheel
  \item Sensors
    \subitem Cameras, speedometer
\end{itemize}

For an email spam filter:
\begin{itemize}
  \item Performace meassure
    \subitem Manimizing false positives
  \item Environment
    \subitem User email account
  \item Actuators
    \subitem Mark as spam, let mail through
  \item Sensors
    \subitem Incoming messages
\end{itemize}

\subsection{Environment types}%
\label{sub:environment_types}
\paragraph{Fully observable vs partially observable}
In a games, such as FIFA or any game, it is possible to oberserve everything,
and perceive all options.
If a robot is playing football in the real world, then the perception of the
environment is constrained by the input of the sensors eg eyes (you do not know
what happens behind your back).

\paragraph{Deterministic vs stochastic}
A deterministic environment is when the coming events in time is only defined
by the current state, and the agent's future actions. Versus when the game is
dependent on some type of randomness, whether it be a deck of cards or dice.
This is very hard to predict.

\paragraph{Episodic vs sequential}
Is every step of the agent independent? Or does it depend on a previous
sequence? A sequence could be a game, where current decisions depend on a 
previous sequence of decisions. In a spam filter the sequence of spam mail does
not matter.
This is a matter of defintion, it is how you see your environment in terms of
your decisions. A game could be episodic, if you choose to look upon it as a
snapshot.

\paragraph{Static vs dynamic}
Is the world changing while the agent is thinking. Solving a rubrics cube or
chess is static. However self-driving is very dynamic, here things are changing
all the time. This a very challenging 


\paragraph{Discrete vs continuous}
Does the environment provid a fixed number of distinct number of states(can they
be enumerated)? Where time is a factor, then time is always continuous, if you
do not choose to look at time in buckets of seconds or minutes.

\paragraph{Single-agent vs multiagent}%
\label{par:single_agent_vs_multiagent}
Is the agent operating by itself?

\subsection{Hierarchy of agent types}%
\label{sub:hieryachy_of_agent_types}
\begin{itemize}
  \item Table-driven agents, it's just like a big lookup table. 
    \subitem The table sizes can become huge. Designing such a table is
    challenging. We need to think about every single case.
  \item Simple reflex agent
    \subitem This is the vacuum cleaner. For every state given the rules, we
    find the rule applying to the current state. It selects its action from the
    current percept only. Implemented through condition-action rules. Example of
    if-then algorithms. For this to work, then the environment must be fully
    observable. This wont then work for a self-driving car.
  \item Agents with memory, internal state to keep track of past states of the
    world
    \subitem We can call this a model-based reflex agent. Internal state:
    aspects of the environment that cannot be currently observed. This is useful
    for partially observable environments.
    In addition to the rule we have a model, a desripton of how the next state
    depends on current state and action.
    We also have a state, a description of the current world state
    the action, is the most recent action.

    We update the state, according to the state, action, percept and the model.
    When matching a rule, the we take into account the state and the rules.
  \item Agents with goals
    \subitem 
  \item Utility 
\end{itemize}


\newpage


\section{Week 8 - Uninformed Search}

\subsection{Formulating search problems}%
\label{sub:formulating_search_problems}

Many problems can be solved through search. Which sequence of actions can get us
to the goal state? This can be solved through search. We might also have a
performance measure of minimizing time. For an example a GPS is a search
problem, which sequence of places gets us to the goal?
\cite[p. 2]{presentation:solving_problems_by_searching}

There can be many different examples of using search as a solution to a problem.
Examples are getting somewhere, we can formulate the problem as:
\begin{itemize}
  \item Start: home
  \item Goal: destination
  \item operators: move one block, turn
\end{itemize}


We can also formulate getting settled in a new apartment as a search problem.
\begin{itemize}
  \item start: item randomly distributed over the place
  \item goal: satisfactory arrangement of items
  \item operators: select item, move item
\end{itemize}

\subsubsection{Searching}%
\label{ssub:search}

A search algorithm will tell us the exact sequence to get us from the start to
the goal. Search strategies are important methods for many approaches to
problem-solving. Search algorithms is a basis for many optimization and planning
methods.

We will consider the problem of designing goal based agent in observable (the
agent always knows the current state), deterministic (that each action we take
has exactly one outcome), discrete (that we have finite number of actions to
choose from), known (such that the agent knows which states are reached by each
action) environments\cite[p. 66]{book:artificial_intelligence_modern_approach}. 


\paragraph{State Space}
Is the initial state, actions and the successor function (or transition model)
defines the state space of the problem.  The set of all states reachable from
initial state by any sequence of actions.  This can be represented as a directed
graph (nodes are states and the edge between nodes are actions).  

\paragraph{Building a goal based agent}

To build a goal based agent, we must answer the questions\cite[p.
15]{presentation:solving_problems_by_searching}:
\begin{itemize}
  \item How to we represent the state of the world?
  \item What is the goal and how can we recognize it?
  \item What \textit{relevant} information do we encode to describe states,
    action and therir effects aand thereby solve the problem.
\end{itemize}

\subsection{Examples of search problems}

How we solve a comic-book maze.
\begin{itemize}[noitemsep]
  \item solution fixed sequence of actions 
  \item Search: process  of looking for the sequence of of actions that reaches
    the goal
  \item Agent can ignore percepts during execution.
\end{itemize}

\paragraph{The maze problem}

To solve a maze by searching we examine the components of a search problem.
\cite[p. 8]{presentation:solving_problems_by_searching}

\begin{itemize}[noitemsep]
  \item Initial state 
    \subitem Entry 
  \item Successor Function
    \subitem Is the result of the doing an action in a state
  \item Goal state 
    \subitem Goal state is the exit.
  \item Path cost
    \subitem Assume that it is a sum of nonnegative \textit{step costs}.
  \item Optimal solution: sequence of actions with the lowest path cost.
\end{itemize}

The optimal solution is the sequence of actions with the \textit{lowest path
cost} for reaching the goal.

\paragraph{Vacation in Romania} 
We are on vacation in Romania, currently we are placed in Arad. However our flight
leaves Bucharest tomorrow. Now we want to find the best way to get to Bucharest.

\begin{itemize}[noitemsep]
  \item Initial state
    \subitem Arad
  \item Successor function 
    \subitem These are the possible states we can move to $S(\text{Arad}) = \{\text{Zerind}, \text{Timisoara}, \text{Sibiu}\}$
  \item Goal state
    \subitem Bucharest
  \item Path cost
    \subitem The cost of the path from the start to the goal, is the sum of edge
    costs.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[page=9,
  width=0.8\textwidth]{../presentations/w8_lec03_search_intro_2021.pdf}
  \label{fig:the_romania_problem}
  \caption{The Romania problem \cite[p.
  9]{presentation:solving_problems_by_searching}}
\end{figure}

\paragraph{Vaccum-cleaner world}
We can define a state for the vaccum world:
\begin{itemize}[noitemsep]
  \item Initial state 
    \subitem Dirty
  \item Goal state
    \subitem All clean
  \item State space:
    \subitem 
    \begin{figure}[H]
      \center
      \includegraphics[width=0.7\textwidth]{./statespace.pdf}
      \caption{The state space of the vacuum cleaner world}
    \end{figure}
  \item Path cost
    \subitem Could be the sum of the amount of electricity used, or any other
    type of utility function.
\end{itemize}

\paragraph{Example: The 8-puzzle}
The state space of the 8-puzzle has a size of 181440 states. The successor
function is the actions of move blank left, right, up, down. The optimal
solution is NP-hard\cite[p. 17]{presentation:solving_problems_by_searching}.

\paragraph{Example: The 8-queens problem}
On chessboard 8 queens are placed. We must place the queens, such that no other
queen can attack each other. 
\begin{itemize}
  \item State space: Is the chessboard
  \item Successor function: depends on how you want to formalize it. It could be
    moving one piece in a direction, such that you have a different
    configuration.
  \item Goal state: Check if any queen is on the same row, column or diagonal.
\end{itemize}

\paragraph{Example: Remove 5 sticks}
Remove exactly 5 of the 17 stick such that the result forms 3
squares \cite[p. 18]{presentation:solving_problems_by_searching}.

\begin{itemize}
  \item States: 
  \item Initial state: The initial state is the state presented with all the
    sticks laid out, and having the 6 squares.
  \item Successor function: Picking up any stick from the board.
  \item Goal test: Check if the remaining sticks form the 3 squares, we need to
    have a function for that.
  \item Path cost: It cost one to remove a stick.
\end{itemize}


\subsection{Tree search}

We expand at the root node(which is the starting state). As we go about
expanding the tree of the search, we maintain a list of unexpanded nodes. This
we call the fringe. For each iteration we expand a node we pick from the fringe.
We keep expanding until we reach the goal state. Our \textbf{goal} is to expand
as few states as possible. This is the case as it takes time and can fill up the
memory of the computer.

An important point is to that \emph{nodes and states} are not the same. A state
is a representation of a physical configuration, while a node is a data
structure that is par of the search tree\cite[p.
19]{presentation:solving_problems_by_searching}

A tree search algorithm works in the following way:
\begin{enumerate}
  \item Initialize the fringe with the starting state
  \item As long a the fringe is not empty:
    \begin{enumerate}
      \item Choose a fringe node to expand
      \item If the node contains the goal state, then return that solution
      \item If not, expand the node and add the children of this node to the
        fringe.
    \end{enumerate}
  \item If we reach the end, terminate having found no solution.
\end{enumerate}
We keep an explored set of the node we already have visited, a node is added to
this everytime we expand it. When we add a node to the fringe, then we check if
it exists, with a higher path cost. If this is the case, then replace it.

\subsection{Search strategies}%
\label{ssub:search_strategies}

A search strategy is defined by the order in which we pick which node to expand
from the fringe. We have some different parameters by, which to judge a search
strategy:
\begin{itemize}[noitemsep]
  \item Completeness
    \subitem Does it find a solution if it exists?
  \item Optimality
    \subitem Does it find the cheapest solution?
  \item Time complexity
    \subitem This represents the number of the node generated aka the time it
    take to run.
  \item Space complexity
    \subitem How many nodes are in memory?
\end{itemize}
We can measure the time and space complexity in terms of:
\begin{itemize}
  \item $b$: maximum branching factor of the search tree
  \item $d$: depth of the least cost (cheapest) solution.
  \item $m$: maximum length of any path in the state space (could be unbounded).
\end{itemize}

\subsubsection{Breadth-first search}%
\label{par:breadth_first_search}
We expand the shallowest node, such that we search in the breath of the tree.
This is done by the fringe being made as a first in-first out queue. Such each
node that we expand, goes to the end of the fringe list.

\begin{description}
  \item[Compleness] Yes
  \item[Optimality] Yes - if the cost is 1 for each step.
  \item[Time complexity] Number of node in a $b$-ary tree of depth $d$: the
    complexity is $O(b^d)$. Where $d$ is the depth of the optimal solution.
  \item[Space complexity] $O(b^d)$
\end{description}

\subsubsection{Depth-first search}%
\label{par:depth_first_search}
We expand the deepest node first, such that we traverse the depth of the tree,
before visiting node higher up. This is done as a last-in-first-out queue.

\begin{description}
  \item[Compleness] Cannot deal with infinte depths, and space with loops. We be
    complete in finite space, if repeats are excluded from the list of visited
    nodes.
  \item[Optimality] No - it returns the first solution that it finds.
  \item[Time complexity] At depth $m$: $O(b^m)$.
  \item[Space complexity] $O(bm)$
\end{description}

\subsubsection{Iterative deepening search}%
\label{par:iterative_deepening_search}
This is a mix of depth first, and breadth-first. We first examine the tree in a
depth-first with the depth of 1 (we cut off node on level 2 and below). If we do
not find out solution we run again with a depth of 2. We again search in a depth
first-fashion. It is like having a depth-first approach, but find an optimal
solution. This way we cannot fall into infinte loops.


\subsubsection{Uniform-cost search}%
\label{par:uniform_cost_search}
We organise the fringe such that all node are sorted according to the cost. Such
that when we expand the root node, then its children B (cost 5) and D (cost 3)
are sorted in the order D, B. We then go on to expand D.

Note that \cite[p. 33]{presentation:solving_problems_by_searching} says that we
should keep expanding nodes in the fringe until we are about to expand the goal
node, and we should \textbf{not} stop when encountering it. Because there might
a hidden better solution in the last unexpanded nodes.



\newpage



\section{Week 9 - Informed Search}%
\label{sec:1_march_informed_search}
The idea of informed search is to give the algorithm hints about the desirability
of different states. Here we can use an evaluation function to rank node and
select the most promising one for expansion. In this way can find the solution
(goal) more efficiently, than just using uninformed search\cite[p.
92]{book:artificial_intelligence_modern_approach}.

We remember from the uniform-cost search, picking the best node (or the most
promising node) to expand first. This we call best-first search, where use a
evaluation function $f(n)$.
\begin{definition}
An \textbf{evaluation function } is a cost estimate, the evaluation function defines our
search strategy (which node we pick first).
\end{definition}

\paragraph{Heuristic function}%
\label{par:heuristic_function}

\begin{definition}
  A \textbf{heuristic function} $h(n)$ estimates the cost of reaching a goal
  from node $n$. The heuristic function is a type of evaluation function, that
  is often used in best-first search.
\end{definition}

As an example we can use the heuristic function in the Romaina problem.
Presented here with the heuristics:
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth,
  page=4]{../presentations/w9_lec04_informed_search_sanja_2021.pdf}
  \caption{Heuristics for the Romania problem}%
  \label{fig:romania_problem_heuristics}
\end{figure}

Figure \ref{fig:romania_problem_heuristics} shows the straight line distance
from the town to Bucharest. This is not equal to the shortest distance on the
graph. The heuristic is merely to inform our descisions in the heuristic
function $h(n)$, when deciding which node to expand.



\subsection{Informed Search Algorithms}%
\label{sub:informed_search_algorithms}

There multiple types for informed search algorithms. In this section we outline
these search algorithms.

\subsubsection{Greedy best-first search}%
\label{ssub:greedy_best_first_search}
Using the greedy best-first search algorithm we expand the node, which has the
\emph{lowest value} of the heuristic function $h(n)$.

\paragraph{Properties}%
\label{par:properties}

The greedy-best first search has some properties and pitfalls. The greedy
best-first is \textbf{not complete} as it can get stuck in loops, where it jumps
between two nodes, with a lower value than the two it jumps between. It is
\textbf{not optimal} as it can make some not optimal descisions. As an example
it will make the journey to Bucharest in the order: Arad, Sibiu, Fagaras,
Bucharest, while the more optimal solution is: Arad, Sibiu, Rummucu Vilcea,
Pitesti, Bucharest. The \textbf{time complexity}  the worst case is: $O(b^m)$,
and best case is $O(bd)$ is the heuristic function is 100\% accurate\cite[p.
12]{presentation:informed_search}.

\subsubsection{A* search}%
\label{ssub:a_search}

As the greedy best-first search is flawed, we can change the \emph{evaluation
function} $f(n)$. We want to change the evaluation function $f(n)$ to more
accurately represent the estimated total cost of the path from the node $n$ to
the goal.
\begin{definition}
  We define A* where:
  $$f(n) = g(n) + h(n)$$
\end{definition}
\begin{itemize}
  \item $g(n)$: cost of getting to $n$, the path cost.
  \item $h(n)$: estimated cost from $n$ to the goal, this is our heuristic.
\end{itemize}

The algorithm is identical to uniform-cost, it just uses $g(n) + h(n)$ instead
of just $g(n)$\cite[p. 93]{book:artificial_intelligence_modern_approach}.

\begin{definition}
  An \textbf{admissible heuristic} never overestimates the cost to reach the
  goal (it is optimistic). 
\end{definition}
An example of an admissible heuristic is using the straight line distance from
$n$ to the goal. This is the case because the straight line distance is always
\emph{less} than the true distance to get somewhere. If $h(n)$ is admissible,
then \textbf{A* is optimal}\cite[p. 21]{presentation:informed_search}.
A* is optimally efficient, no other tree-based algorithm that use the same
heuristic can expand fewer nodes and still be guaranteed to find the optimal
solution.

\subsection{Designing a Heuristic Function}%
\label{sub:designing_a_heuristic_function}

We look at getting a heuristic function for the 8-puzzle.
\begin{itemize}
  \item $h_1(n)$: number of misplaced tiles.
  \item $h_2(n)$: total Manhattan distance of the tiles form their goal
    positions
\end{itemize}

We now want to determine if the heuristics $h_1$ and $h_2$ are admissible. The
results of this is for a goal state:
\begin{itemize}
  \item $h_1(start) = 8$
  \item $h_2(start) = 3+1+2+2+2+3+3+2 = 18$
\end{itemize}

The results of the two heuristics is less than the actual cost, thus the
heuristics are optimistic, and this they are admissible.

\begin{definition}
  A \textbf{dominant heuristic} the heuristic which dominates the other in A*
  search. This is the case if $h_2(n) \geq h_1(n)$ for all $n$.
\end{definition}

\paragraph{Heuristics from relaxed problems}%
\label{par:heuristics_from_relaxed_problems}

\begin{definition}
A \textbf{relaxed problem} is a problem with fewer restrictions on the actions.
\end{definition}

The optimal cost to nodes in the relaxed problem is an admissible heuristic for
the original problem. An example of a relaxed heuristic is the 8-puzzle is
relaxed such that a tile can move anywhere on the board, then $h_1(n)$ gives the
shortest solution (a tile's shortest path to its right place). We can also relax
the rules of the game, such that a tile can move to any adjacent squares, then
$h_2(n)$ give the shortest solution.

Looking at the typical search cost for the 8-puzzle (average number of nodes
expanded for different solution depths). 

If the solution has a depth $d$ of $d = 12 $. Then:
\begin{itemize}
  \item $IDS = 3644035$ nodes
  \item $A*(h_1) = 227 $ nodes
  \item $A*(h_2) = 73$ nodes
\end{itemize}

If the solution has a depth $d$ of $d = 24 $. Then:
\begin{itemize}
  \item $IDS = 54,000,000,000$ nodes
  \item $A*(h_1) = 39,135 $ nodes
  \item $A*(h_2) = 1,641$ nodes
\end{itemize}

So there is a clear benefit to choosing the right heuristic\cite[p.
31]{presentation:informed_search}. 

However if there is no dominant heuristic and we have a collection of admissible
heuristics $h_1(n),h_2(n), \dots , h_m(n)$. Then we can combine them with:
\[h(n) = \max\{h_1(n),h_2(n), \dots , h_m(n) \}\]

\subsection{Weighted A* search}%
\label{sub:weighted_a_search}

The idea of weighted A* search is to speed up the search at the expense of
optimality. So we take an admissible heuristic, and inflate it by $\alpha > 1$,
and then person the A* search as usual. By using this approach we expand fewer
nodes, but we might not reach the solution in the most optimal way. The cost
will be at most $\alpha$.



\newpage
\section{Week 10 - Local Search}
We can formulate problems in terms of optimization. This is that we do not have
a start state, and we do not care about the path to a solution. We have an
objective function that us about the quality of a possible solution, and we want
to find a good solution by minimizing or maximizing the value of the function.

There is three approaches to local search.

\subsection{Hill-climbing (greedy) search}
Here the crux is: keep a single "current state" and try to locally improve it.
You can like it to "climbing mount Everest in thick fog with amnesia".

If we look at a puzzle, it should be 1, 2, 3, 4, 5, 6, 7, 8 going in a clockwise
circle our function is $$f(n) = -(\text{number of tiles out of place})$$.
We want to find the best solution, the one closest to what we think we want.

In each iteration we probe, for which move increases our evaluation function.

The psudeocode the algorithm is:

\begin{lstlisting}[inputencoding=utf8/latin1, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
current = starting state
loop:
  next = highest value successor of current
  if (value(next) < value(current))
    return current
  else 
    current = next
\end{lstlisting}

Because it just probes for best solutions in the visible space, it can get stuck
in local optima.

An idea for escaping local optima is to iteratively do hill-climbing, each time
we initialize with a random condition. If subsequent searches improve, then a
stored state is replaced.

\subsection{Simulated annealing search}

The hill climbing will be never go down the hill of global and local maxima. 
The idea is to escape local maxima by allowing some "bad" moved but gradually
decrease the frequency of these bad moves.
\begin{itemize}
  \item Probability of takeing downhill move decreases with number of
    iterations, steepness of downhill move
  \item Controlled by annealing schedule 
\end{itemize}
Inspired by annealing process, as part of the tempering of glass, metal.

The pseudocode for the simulated annealing search:

\begin{lstlisting}[inputencoding=utf8/latin1, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
current = starting state 
for i = 1 to infinity
  next = random successor of current 
  let delta = value(next) - value(current)
  if (delta > 0)
    current = next
  else 
    current = next (with probability exp(delta/T(i)))
\end{lstlisting}

$T$ is gradually decrease to $0$ over some time $t$. We always accept an
improvement (delta greater than 0), but if not we accept with a probability of
$< 1$. We are more likely to pick bad moves early than later.

If the "temperature" $T$ decreases slowly enough, then the simulated annealing
search will find a global optimum with a probability approaching one. This is
not practical, the more bad choices we need, the longer the search takes (we
make fewer good choices in a row).

A better more modern technique is Markov Chain Monte Carlo algorithm for
expolring complicated state spaces.

\subsection{Genetic Algorithms}%
\label{sub:genetic_algorithms}

Each living cell contains \textbf{chromosomes}, which are strings of DNA. These
chromosomes contain \textbf{genes}, which are sections of each strand of DNA.
The genes determine traits of the organism. A \textbf{genotype} is a collection
of genes. A collection of traits is called a \textbf{phenotype}. Reproduction
involdes \textbf{recombination} of genes from parents, and then
\textbf{mutations} (errors) occurs when these genes are copied. \textbf{Fitness}
of an organism is how much it can reproduce before it dies.
We evolve based on "survival of the fittest".\cite[p.  21]{presentation:local_search_algorithms}

A bad idea is to generate random solution and to check if they are correct.

We can take this idea and use it to implement a genetic algorithm (GA).

\begin{lstlisting}[inputencoding=utf8/latin1, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
Generate a set of random solutions
while(solution is not good enough)
  test each solution in the set (rank them)
  prune the bad solution from the set
  duplicate the good solutions
    make small changes to these
\end{lstlisting}

\subsubsection{Encoding a solution}%
\label{ssub:encoding_a_solution}

We need to encode a solution, such that we can make random changes to these.
This is sometimes done as bitstrings (eg. 101011). Where each bit represents
some aspect of the problem. An important point is that we need to be able to
probe each string and get a score it.

\subsubsection{Digging for oil}%
\label{ssub:digging_for_oil}

Along a 1 km stretch of road, we want to choose where to drill for oil.
The search space (or state space) is the set of all possible solutions, in this
case we have a space of \texttt{[0..1000]}. We encode the search space in
bitstring of length 10. We then for each generated string check the fitness
function, as seen on \cite[p. 31]{presentation:local_search_algorithms}.

\subsubsection{Search Space}%
\label{ssub:search_space}

How the space is formed defines the nature of the Genetic Algorithm's
performance. As an example a completely random space is bad for a GA, we there
is no pattern to it and if we have a lot of local maxima, the algorithm can get
stuck in these.

\subsubsection{Reproduction in a GA}%
\label{ssub:reproduction_in_a_ga}

We look at duplicating the good solutions (ranked according to the fitness
function), and making changes to these. Relying on random mutation is not
necessarily a good idea. Therefore we introduce crossover in reproduction to get
better results.

The idea is: take two high scoring strings and with some probability (the
crossover rate) combine them into two new bitstrings. Each of these offspring
can also be changed randomly (remember mutations).

In selecting the parents of the bitstrings, we want to pick some of the best
strings. One approach is the "Roulette Wheel" approach.
\begin{itemize}
  \item We start by adding the fitness of the chromosomes.
  \item Then we generate a random number $R$ in this range.
  \item Then the first chromosome in the population that is $\geq R$, is picked.
\end{itemize}

An example of roulette selection is seen here:


\begin{figure}[H]
  \centering
  \includegraphics[page=41,
  width=0.8\textwidth]{../presentations/w10_lec05_local_search_sanja_2021.pdf}
  \label{fig:roulette_wheel}
  \caption{The roulette selection}
\end{figure}

A way of combine the two parents, when we have picked them is
\emph{recombination}.

\begin{definition}
  \textbf{Recombination} is when we with some high probability $0.8$ to $0.95$
  choose to replace the rest of the digits, with the content from the other. 
\end{definition}

\begin{definition}
  The \textbf{crossover point} is the random single point from the strings are
  swapped.
\end{definition}

We now also introduce mutations. Mutations occurs for each digit with some small
probability between $0.1$ and $0.001$.

\subsubsection{The algorithm}%
\label{ssub:the_algorithm}

\begin{lstlisting}[inputencoding=utf8/latin1, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
Generate a population of random chromosomes
while(solution is not good enough)
  calculate the fitness for each chromosome
  for each chromosome
    use the roulette selection to select pairs
    generate offspring of this with crossover and mutation
  new population as been produced
\end{lstlisting}


\subsubsection{Paramenters of a GA}%
\label{ssub:paramenters_of_a_ga}

We of course need to set the population size (the number of chromosomes) the
mutation rate $m$ and the crossover rate $c$.
These values have been tuned based on the results. There exists no good general
theory to deduce good values.
These best solution is the fittest of the population solutions.




\newpage

\section{Week 11 - Adversarial Search}%
\label{sub:_adversarial_search}

In games there are different types of environments.

\begin{table}[h]
  \centering
  \begin{tabular}{p{3.5cm}   l  l} 
     & \textbf{Deterministic}  & \textbf{Stochastic}  \\\midrule
    Perfect information \newline (fully obeservable) & Chess, checkers, go  &
    Backgammon, monopoly \\ \midrule
    Imperfect information \newline 
    (partially observable) & Battleships & Scrabble, bridge
  \end{tabular}
  \caption{Types of game environments}
  \label{tab:label}
\end{table}

We can also call deterministic, fully observable game zero-sum games. A games
often has a state space that is easy to represent, as the nature of a game is
somewhat restricted. Agents in a game is usually restricted to a small number of
action whose outcomes are defined by precise rules.

This is opposition to physical games which has complex descriptions, and the
rules can be quite ambiguous. Games are interesting because they are hard
problems to solve. The branching factor of chess is about $35$. Each player
makes about $50$ moves. The search tree is therefore $35^{100}$. There is about
$10^{40}$ total distinct nodes in the search tree.  Games are also interesting
because some instate a time limit, such we cannot search for the optimal
solution in infinity. We therefore need to make \emph{some} solution, when we
cannot calculate the optimal solution. 

\begin{definition}
  \textbf{Pruning} allows us to ignore parts of the search tree, that have no
  impact on the final choice of the algorithm. Thus reducing complexity.
\end{definition}

\subsection{A small game}%
\label{sub:a_small_game}

We can look at two players \textsc{min} and \textsc{max}. These players each
take turns. We can formulate a search problem with the following contents, where
$s$ is the state of the game:

\begin{itemize}
  \item $S_0$: The initial state of the game 
  \item $\textsc{Player}(s)$: Which player has the ability to move in the game.
  \item $\textsc{Actions}(s)$: Return the set of legal moves in the current
    state of the game.
  \item $\textsc{Result}(s,a)$: The transition model, which defines the result
    of a move $a$.
  \item $\textsc{Terminal-Test}(s)$: A terminal test, which is true if the game
    has ended. States where the game has ended is called terminal states.
  \item $\textsc{Utility}(s,p)$: A utility function defines the numeric value
   for a game that ends in terminal state $s$ for player $p$.
\end{itemize}

In a game $\textsc{Actions}(s)$ and $\textsc{Result}(s,a)$ forms the game tree.
Each leaf node of the tree has a utility score, as computed by the utility
function \textsc{Utility}.


\subsection{Decisions in games}%
\label{sub:decisions_in_games}

Since we have a game, with an opponent (in our case \textsc{Min}), we cannot
simply search our way to the result, we are influenced by the actions of our
opponent. Therefore \textsc{Max} must have a contingent strategy.

\begin{definition}
  A \textbf{contingent strategy} is a strategy which is contingent on the moves
  of an opponent.
\end{definition}

We look at a game tree.

\begin{figure}[H]
  \centering
  \includegraphics[page=9,
  width=0.8\textwidth]{../presentations/w11_lec06_adversarial_search_sanja_2021.pdf}
  \label{fig:simple_game}
  \caption{An abstract two player game \cite[p.
  9]{presentation:games_and_adversarial_search}}
\end{figure}

Here we see that \textsc{Max} has the possible moves of $a_1, a_2, a_3$. If
\textsc{Max} chooses $a_1$, then \textsc{Min} has the choices of $a_{11},
a_{12}, a_{13}$. We get the optimality of each node with the
\textsc{Minimax}$(n)$ function. \textsc{Max} wants to move to the state with the
highest value (maximizing the chance of winning). \textsc{Min} wants to minimize
the value of the \textsc{Minimax}$(s)$ function. The logic for the
\textsc{Minimax} goes:

\begin{lstlisting}[inputencoding=utf8/latin1, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
Minimax(state) =
  if (Terminal-Test)
    Utility(state)
  if(Player(s) = MAX) {
    max{Minimax(successors(state))}
  }
  if(Player(s) = MIN {
    min{Minimax(successors(state))}
  }
\end{lstlisting}

The \textsc{Minimax} strategy is optimal against an optimal opponent. 

\paragraph{Coin game}%
\label{par:coin_game}

We have a stack of $N$ coins, each player takes 1, 2 or 3 coins from the stack,
the one who takes the last coin loses. The game is defined:

\begin{itemize}
  \item Initial state: The number of coins in the stack.
  \item Operators:
    \begin{enumerate}
      \item Remove one coin 
      \item Remove two coins 
      \item Remove three coins
    \end{enumerate}
  \item Terminal test: There are no coins left on the stack
  \item Utility function: F(S)
    \subitem F(S) = 1 if MAX wins, 0 if MIN wins
\end{itemize}

\subsection{Alpha-Beta Pruning}%
\label{sub:alpha_beta_pruning}

The idea is to improve the performance of the minimax algorithm through
alpha-beta pruning. A quote is: \textit{"If you have an idea that is surely bad,
don't take the time to see how truly awful it is"}.

Through Alpha-Beta pruning we avoid processing the subtrees that will have no
effect on the result. We keep two parameters: $\alpha$, which is the best value
for \textsc{Max} seen so far, and $\beta$, which is the best value for
\textsc{Min}. $\alpha$ is used in \textsc{Min} nodes, and is assigned in
\textsc{Max} nodes. $\beta$ is used in \textsc{Max} nodes, and is assigned in
\textsc{Min} nodes.

\begin{lstlisting}[inputencoding=utf8/latin1, basicstyle=\footnotesize, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
traverse the tree in a depth-first order
At MAX node n, alpha(n) = max value found  (alpha start at -inf)
At MIN node n, beta(n) = min value found so far (beta value starts at inf)
\end{lstlisting}

\emph{Beta cutoff}: stop searching below \textsc{Max} node $N$, if
\textsc{alpha($N$)} $>=$ \textsc{beta($i$)} for a \textsc{Min} node ancestor $i$ of $N$.

\emph{Alpha cutoff}: stop searching below \textsc{Min} node $N$ if
\textsc{beta($N$)} $<=$ \textsc{alpha($i$)} for a MAX node ancestor $i$ of $N$

It is possible to compute the exact minimax decision without expanding every
node in the game tree.

So looking at the example in the presentation. We are looking at the tree from
the MAX position. We look in a depth of 2, we want to find the best way to go.
So in the first sub tree we traverse the whole thing, and find that 3 is the
lowest value. So we want to look in the other subtrees to find if there exists a
higher value than 3. 

In subtree 2, we find a 2. Therefore there is no better way in this subtree, as
we want to find something higher or equal to 3. The rest is pruned.

In subtree 3, we first find a 14, we continue, finding 5, however in the last
leaf node, we find a 2.

We thus want to pick the first subtree as this is worst for our opponent.

\newpage

\section{Week 12 - Constraint Satisfaction Problems}%
\label{sec:23_march_constraint_satisfaction_problems}

With constraint satisfaction we have a problem with some \emph{variables},
\emph{a domain} and some \emph{constraints}. An example of constraint
satisfaction is the $N$-queens problem. Where we have the constraint of we have
to use $N$-queens, as we have to fit the size of the board. And the obvious
constraints of that we cannot place others horizontally, vertically and
diagonally from the placement of the queen.

Constraint satisfaction is a way to propagate constraint imposed by the one
queen to the others. 

\subsection{Example problems}%
\label{sub:example}

\paragraph{N-Queens}

A formal definition of $N$-queens:

\begin{itemize}
  \item Variables
    \subitem $X_{ij}$ - this is a variable of the board (some placement).
  \item Domains 
    \subitem $\{0,1\}$ - either the placement on the board has a queen or not.
  \item Constraints 
    \subitem $\sum_{i,j}X_{ij} = N$ - for all $i,j$ there must be exactly $N$
    queens.
    \subitem $(X_{ij}, X_{ik}) \in \{(0,0), (0, 1), (1,0)\}$ - two queens cannot
    be on the same column. This is the case as the $i$s are the same.
    \subitem $(X_{ij}, X_{kj}) \in \{(0,0), (0, 1), (1,0)\}$ - two queens cannot
    be on the same row. This is the case the $j$s are the same.
    \subitem $(X_{ij}, X_{i+k, j + k}) \in \{(0,0), (0, 1), (1,0)\}$ - two
    cannot be on the same downward diagonal.
    \subitem $(X_{ij}, X_{i+k, j - k}) \in \{(0,0), (0, 1), (1,0)\}$ - two
    queens cannot be on the same upward diagonal.
\end{itemize}

\paragraph{Cryptarithmetic}

\begin{itemize}
  \item Variables 
    \subitem T, W, O, F, U, R, $X_1$, $X_2$.
  \item Domains
    \subitem $\{0, 1, 2, \dots, 9\}$
  \item Constraints
    \subitem $O + O = R + 10 *X_1$
    \subitem $W+W + X_1 = U + 10 * X_2$
    \subitem $T + T + X_2 = O + 10 * F$
    \subitem All the letters has to have different values.
    \subitem $T \neq 0$, $F \neq 0$
\end{itemize}

\subsection{Search formulation}%
\label{sub:search_formulation}

\begin{itemize}
  \item States: 
    \subitem Variables and values assigned so far.
  \item Initial state: 
    \subitem The empty board.
  \item Action: 
    \subitem We choose some unassigned variable and assign it to a value that
    does not violate any constraints. This should fail if there are no valid
    assignments.
  \item Goal tests:
    \subitem The current assignment is complate and satisfies all constraints.
\end{itemize}

This is a good naïve approach. Then for $n$ variables the depth of the solution
is $n$. There are $m$ possible values for any variable, there is thus $n! \cdot
m^n$ paths in the search tree, this yields an enormous search space.

\paragraph{Backtracking search}%
\label{par:backtracking_search}

An interesting point is that assignments are commutative. [WA = red then NT =
green] is the same as [NT = green then WA = red], the order does not matter.  We
only need to consider assignments to a single variable at each level (i.e., we
fix the order of assignments) – Then there are only $m^n$ leaves ($n$ – number
of variables and $m$ is the number of leaves).
Depth-first search for constraint satisfaction problems with single-variable
assignment is called backtracking search.

Then the exploration of a node fails then the search backtracks to an unexplored
state.

We look at the \textsc{CSP-Backtracking} algorithm:
\begin{lstlisting}[inputencoding=utf8/latin1,basicstyle=\small, keywordstyle=\color{blue}\bfseries, rulecolor=\color{black}]
CSP-Backtracking(PartialAssignment A)
  if A is complete then return A
  
  X = unassigned variable
  D = select an ordering for the domain of X
  
  for v in D
    if v is consistet with A then 
      Add (X=v) to A 
      result = CSP-Backtracking(A)
      if result != failure then return result 
      remove (X=v) from A
  return failure
  
\end{lstlisting}

To backtracking there some relevant questions to ask:

\begin{itemize}
  \item Which variable should be assigned next? It is not necessarily the best
    approach to pick the next variable based on the sequence.
    \subitem We can choose the \emph{most constrained variable}, such the we
    choose the variable with the fewest legal values. This is called the minimum
    remaining values heuristic.

    We can also choose the \emph{most constraining variable}, which imposes the
    most constraints on the remaining varaibles. This is a tie-breaker among
    most constrained variables, this is seleced among the varaiables with the
    smallest remaining domains. 
  \item In what order should its values be tried?
    \subitem We can choose the least constraining variable. Which is the value
    the rules out the fewest in the remaining variables.
  \item Can we detect inevitable failure early? 
    \subitem Keep track of remaining legal values for unassigned variables. We
    terminate search when any variable has no legal values.
\end{itemize}

\paragraph{Constraint propagation}%
\label{par:constraint_propagation}

Forward checking propagates information form assigned to unassigned variables,
but does not provide early detection for all failures.

Arc consistency is a good way of keeping track of which values are allowed in
related nodes.

\newpage
\section{Week 15 - Probability}%
\label{sec:12_march_probability}

\newpage
\section{Week 16 - Bayesian Networks}%
\label{sec:20_march_bayesian_networks}

\newpage
\section{Week 17 - Hidden Markov Models}%
\label{sec:27_march_hidden_markov_models}

\newpage
\section{Week 18 - Intro to Machine Learning}%
\label{sec:}

\newpage
\section{Lab 1 - Week 7}%
\label{sec:lab_week_7}

\subsection{TABEL-DRIVEN-AGENT}%
\label{sub:tabel_driven_agent}

\paragraph{Run the module}
The program prints:

\paragraph{The percept }

\newpage
\section{Lab 2 - Week 8}%
\label{sec:lab_week_8}

\begin{enumerate}
  \item Successor nodes are inserted at front of the fringe (successor list) as
    a node is expanded. Is this a breadth (FIFO) or depth-first search (LIFO)?
    \subitem This is depth-first search. We put the successor node in front, and
    examine these first.

  \item For goal J, give the fringe (successor list) after expanding each node.
    \subitem The fringe list, should be empty, when we have found the goal J.
    However if we just expand the tree, going depth-first with LIFO then we
    would have: 
    \begin{center}
      J I H G F C E D B A 
    \end{center}
  \item What is the effect of inserting successor nodes at the end of the fringe
    as a node is expanded? A depth or breadth-first search?
    \subitem The effect of this is a breadth-first search.
  \item For goal J, give the fringe (successor list) after expanding each node
    \subitem A, B, C, D, E, F , G, H, I, J
\end{enumerate}

\newpage
\section{Lab 3 - Week 9}%
\label{sec:lab_week_9}


\newpage
\section{Lab 4 - Week 10}



\newpage
\section{Lab 5 - Week 11}


\newpage
\section{Lab 6 - Week 12}

\newpage
\printbibliography

\end{document}

